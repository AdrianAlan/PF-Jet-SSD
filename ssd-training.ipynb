{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSD Jet Detection Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setGPU: Setting GPU to: 0\n"
     ]
    }
   ],
   "source": [
    "# Import GPU libs\n",
    "\n",
    "import setGPU\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other imports\n",
    "\n",
    "import numpy as np\n",
    "import simplejson as json\n",
    "\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set presentation settings\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.ticker as tick\n",
    "\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import SymLogNorm\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "matplotlib.rcParams[\"figure.figsize\"] = (16.0, 6.0)\n",
    "\n",
    "matplotlib.rcParams['font.family'] = 'sans-serif'\n",
    "matplotlib.rcParams['font.sans-serif'] = ['Anonymous Pro for Powerline']\n",
    "\n",
    "matplotlib.rcParams[\"axes.spines.left\"] = True\n",
    "matplotlib.rcParams[\"axes.spines.top\"] = True\n",
    "matplotlib.rcParams[\"axes.spines.right\"] = True\n",
    "matplotlib.rcParams[\"axes.spines.bottom\"] = True\n",
    "matplotlib.rcParams[\"axes.labelsize\"] = 16\n",
    "matplotlib.rcParams[\"axes.titlesize\"] = 14\n",
    "\n",
    "matplotlib.rcParams[\"xtick.top\"] = True\n",
    "matplotlib.rcParams[\"ytick.right\"] = True\n",
    "matplotlib.rcParams[\"xtick.direction\"] = \"in\"\n",
    "matplotlib.rcParams[\"ytick.direction\"] = \"in\"\n",
    "matplotlib.rcParams[\"xtick.labelsize\"] = 10\n",
    "matplotlib.rcParams[\"ytick.labelsize\"] = 10\n",
    "matplotlib.rcParams[\"xtick.major.size\"] = 10\n",
    "matplotlib.rcParams[\"ytick.major.size\"] = 10\n",
    "matplotlib.rcParams[\"xtick.minor.size\"] = 5\n",
    "matplotlib.rcParams[\"ytick.minor.size\"] = 5\n",
    "matplotlib.rcParams[\"xtick.minor.visible\"] = True\n",
    "\n",
    "matplotlib.rcParams[\"lines.linewidth\"] = 2\n",
    "\n",
    "matplotlib.rcParams[\"legend.fontsize\"] = 14\n",
    "\n",
    "with open('../data/palette.json') as json_file:\n",
    "    color_palette = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssd.generator import DataGenerator\n",
    "from ssd.keras_ssd7 import build_model\n",
    "from ssd.keras_ssd_loss import SSDLoss\n",
    "from ssd.ssd_input_encoder import SSDInputEncoder\n",
    "from ssd.ssd_output_decoder import decode_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration parameters\n",
    "\n",
    "SAVE_PATH = '/data/adpol'\n",
    "MODEL_NAME = 'ceva-cms-jet-ssd'\n",
    "DATA_SOURCE = '/eos/user/a/adpol/ceva'\n",
    "TRAINING_EPOCHS = 20\n",
    "SPLIT = [0.1, 0.1, 0.1]\n",
    "MAX_EVENTS = 100\n",
    "\n",
    "classes = ['background', 'b', 'h', 'W', 't', 'q']\n",
    "\n",
    "img_height = 452 # Pixel height\n",
    "img_width = 340 # Pixel width\n",
    "img_channels = 1 # Number of channels\n",
    "n_classes = 4 # Number of target classes\n",
    "\n",
    "# Set this to your preference (maybe `None`). The current settings transform the input pixel values to the interval `[-1,1]`.\n",
    "intensity_mean = None \n",
    "intensity_range = None\n",
    "\n",
    "# An explicit list of anchor box scaling factors. If this is passed, it will override `min_scale` and `max_scale`.\n",
    "scales = [0.16, 0.4, 0.6, 0.8, 0.96]\n",
    "\n",
    "# The list of aspect ratios for the anchor boxes\n",
    "aspect_ratios = [1.0]\n",
    "two_boxes_for_ar1 = True # Whether or not you want to generate two anchor boxes for aspect ratio 1\n",
    "steps = None # In case you'd like to set the step sizes for the anchor box grids manually; not recommended\n",
    "offsets = None # In case you'd like to set the offsets for the anchor box grids manually; not recommended\n",
    "clip_boxes = True # Whether or not to clip the anchor boxes to lie entirely within the image boundaries\n",
    "variances = [1.0, 1.0, 1.0, 1.0] # The list of variances by which the encoded target coordinates are scaled\n",
    "normalize_coords = True # Whether or not the model is supposed to use coordinates relative to the image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "model = build_model(image_size=(img_height, img_width, img_channels),\n",
    "                    n_classes=n_classes,\n",
    "                    mode='training',\n",
    "                    l2_regularization=0.0005,\n",
    "                    scales=scales,\n",
    "                    aspect_ratios_global=aspect_ratios,\n",
    "                    aspect_ratios_per_layer=None,\n",
    "                    two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                    steps=steps,\n",
    "                    offsets=offsets,\n",
    "                    clip_boxes=clip_boxes,\n",
    "                    variances=variances,\n",
    "                    normalize_coords=normalize_coords,\n",
    "                    subtract_mean=intensity_mean,\n",
    "                    divide_by_stddev=intensity_range)\n",
    "\n",
    "# Instantiate an Adam optimizer and the SSD loss function and compile the model\n",
    "adam = Adam(lr=0.00001, beta_1=0.7, beta_2=0.9, epsilon=1e-08, decay=0.0)\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "\n",
    "model.compile(optimizer=adam, loss=ssd_loss.compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data generator\n",
    "\n",
    "hdf5_dataset_paths_list = []\n",
    "\n",
    "for i in np.arange(0, 10*SPLIT[0], dtype=np.int16):\n",
    "    for j in ['bb', 'tt', 'WW', 'hh']:\n",
    "        hdf5_dataset_paths_list.append('%s/RSGraviton_%s_NARROW_%s.h5' % (DATA_SOURCE, j, i))\n",
    "\n",
    "train_dataset = DataGenerator(hdf5_dataset_paths=hdf5_dataset_paths_list, max_size=MAX_EVENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data generator\n",
    "\n",
    "hdf5_dataset_paths_list = []\n",
    "\n",
    "for i in np.arange(10*SPLIT[0], 10*(SPLIT[0]+SPLIT[1]), dtype=np.int16):\n",
    "    for j in ['bb', 'tt', 'WW', 'hh']:\n",
    "        hdf5_dataset_paths_list.append('%s/RSGraviton_%s_NARROW_%s.h5' % (DATA_SOURCE, j, i))\n",
    "\n",
    "val_dataset = DataGenerator(hdf5_dataset_paths=hdf5_dataset_paths_list, max_size=MAX_EVENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the training dataset:\t   400\n",
      "Number of images in the validation dataset:\t   400\n"
     ]
    }
   ],
   "source": [
    "train_dataset_size = train_dataset.get_dataset_size()\n",
    "val_dataset_size = val_dataset.get_dataset_size()\n",
    "\n",
    "print(\"Number of images in the training dataset:\\t{:>6}\".format(train_dataset_size))\n",
    "print(\"Number of images in the validation dataset:\\t{:>6}\".format(val_dataset_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an encoder that can encode ground truth labels into the format needed by the SSD loss function.\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "predictor_sizes = [model.get_layer('classes4').output_shape[1:3],\n",
    "                   model.get_layer('classes5').output_shape[1:3],\n",
    "                   model.get_layer('classes6').output_shape[1:3],\n",
    "                   model.get_layer('classes7').output_shape[1:3]]\n",
    "\n",
    "ssd_input_encoder = SSDInputEncoder(img_height=img_height,\n",
    "                                    img_width=img_width,\n",
    "                                    n_classes=n_classes,\n",
    "                                    predictor_sizes=predictor_sizes,\n",
    "                                    scales=scales,\n",
    "                                    aspect_ratios_global=aspect_ratios,\n",
    "                                    two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                                    steps=steps,\n",
    "                                    offsets=offsets,\n",
    "                                    clip_boxes=clip_boxes,\n",
    "                                    variances=variances,\n",
    "                                    matching_type='multi',\n",
    "                                    pos_iou_threshold=0.5,\n",
    "                                    neg_iou_limit=0.3,\n",
    "                                    normalize_coords=normalize_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the generator handles that will be passed to Keras' `fit_generator()` function.\n",
    "\n",
    "train_generator = train_dataset.generate(batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         label_encoder=ssd_input_encoder,\n",
    "                                         returns={'processed_images',\n",
    "                                                  'encoded_labels'})\n",
    "\n",
    "val_generator = val_dataset.generate(batch_size=batch_size,\n",
    "                                     shuffle=True,\n",
    "                                     label_encoder=ssd_input_encoder,\n",
    "                                     returns={'processed_images',\n",
    "                                              'encoded_labels'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available? True\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "\n",
    "print('GPU Available? %s' % (len(K.tensorflow_backend._get_available_gpus()) > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(filepath='%s/%s.h5' % (SAVE_PATH, MODEL_NAME),\n",
    "                                   monitor='val_loss',\n",
    "                                   verbose=0,\n",
    "                                   save_best_only=True,\n",
    "                                   save_weights_only=True,\n",
    "                                   mode='auto',\n",
    "                                   period=1)\n",
    "\n",
    "csv_logger = CSVLogger(filename='%s/%s.csv' % (SAVE_PATH, MODEL_NAME),\n",
    "                       separator=',',\n",
    "                       append=False)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                               min_delta=0.0,\n",
    "                               patience=10,\n",
    "                               verbose=0)\n",
    "\n",
    "reduce_learning_rate = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                         factor=0.2,\n",
    "                                         patience=8,\n",
    "                                         verbose=0,\n",
    "                                         min_delta=0.001,\n",
    "                                         cooldown=0,\n",
    "                                         min_lr=0.00001)\n",
    "\n",
    "callbacks = [model_checkpoint,\n",
    "             csv_logger,\n",
    "             early_stopping,\n",
    "             reduce_learning_rate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4/4 [==============================] - 194s 49s/step - loss: 33.6100 - val_loss: 33.2547\n",
      "Epoch 2/20\n",
      "4/4 [==============================] - 12s 3s/step - loss: 33.1258 - val_loss: 32.6958\n",
      "Epoch 3/20\n",
      "4/4 [==============================] - 11s 3s/step - loss: 32.6375 - val_loss: 32.1571\n",
      "Epoch 4/20\n",
      "4/4 [==============================] - 11s 3s/step - loss: 32.1222 - val_loss: 31.6266\n",
      "Epoch 5/20\n",
      "4/4 [==============================] - 12s 3s/step - loss: 31.6797 - val_loss: 31.1199\n",
      "Epoch 6/20\n",
      "4/4 [==============================] - 12s 3s/step - loss: 31.1870 - val_loss: 30.6051\n",
      "Epoch 7/20\n",
      "4/4 [==============================] - 12s 3s/step - loss: 30.7139 - val_loss: 30.1445\n",
      "Epoch 8/20\n",
      "4/4 [==============================] - 11s 3s/step - loss: 30.2466 - val_loss: 29.6317\n",
      "Epoch 9/20\n",
      "4/4 [==============================] - 11s 3s/step - loss: 29.7824 - val_loss: 29.1578\n",
      "Epoch 10/20\n",
      "4/4 [==============================] - 11s 3s/step - loss: 29.3267 - val_loss: 28.6717\n",
      "Epoch 11/20\n",
      "4/4 [==============================] - 11s 3s/step - loss: 28.8755 - val_loss: 28.2204\n",
      "Epoch 12/20\n",
      "4/4 [==============================] - 11s 3s/step - loss: 28.4280 - val_loss: 27.7723\n",
      "Epoch 13/20\n",
      "4/4 [==============================] - 11s 3s/step - loss: 28.0013 - val_loss: 27.3138\n",
      "Epoch 14/20\n",
      "4/4 [==============================] - 11s 3s/step - loss: 27.5809 - val_loss: 26.8824\n",
      "Epoch 15/20\n",
      "4/4 [==============================] - 12s 3s/step - loss: 27.1569 - val_loss: 26.4761\n",
      "Epoch 16/20\n",
      "4/4 [==============================] - 11s 3s/step - loss: 26.7492 - val_loss: 26.0587\n",
      "Epoch 17/20\n",
      "4/4 [==============================] - 11s 3s/step - loss: 26.3629 - val_loss: 25.6502\n",
      "Epoch 18/20\n",
      "4/4 [==============================] - 11s 3s/step - loss: 25.9459 - val_loss: 25.2617\n",
      "Epoch 19/20\n",
      "4/4 [==============================] - 12s 3s/step - loss: 25.5773 - val_loss: 24.8681\n",
      "Epoch 20/20\n",
      "4/4 [==============================] - 12s 3s/step - loss: 25.1860 - val_loss: 24.5150\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(generator=train_generator,\n",
    "                              use_multiprocessing=False,\n",
    "                              validation_data=train_generator,\n",
    "                              steps_per_epoch=int(np.floor(train_dataset_size/batch_size)),\n",
    "                              validation_steps=int(np.floor(train_dataset_size/batch_size)),\n",
    "                              epochs=TRAINING_EPOCHS,\n",
    "                              workers=0,\n",
    "                              callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
